---
title: "Notes on A Probability Path (Resnick)"
author: "Archimond"
date: "`r Sys.Date()`"
output:
  html_document:
    #toc: true
    toc_depth: 4
    number_sections: true
    css: styles.css
    mathjax: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Pre
These are my notes and exercise records while learning *A Probability Path* by Resnick.  
After finishing Chapters 1–3, I decided to start writing things down more systematically.  
Here is the protocol I follow for exercises:

- I write down anything I can, even if I know something may be wrong.
- I compare with the official solution and locate what went wrong.
- I close the solution and redo the exercise until I can complete it **entirely** on my own.

I record both my first idea and the final correct solution here.

## Chap 1

## Chap 2

## Chap 3

## Chap 4, Independence

### Summary
Firstly, we defined the definition of independence (of 2 events)

Then:

* The independence of finite events
* The independence of finite classes (collection of events)
* The independence of arbitrary number of classes, by focusing of all the finite subset
* Further, if the independent classes are $\pi$-system, then the $\sigma$-field then generated are also independent

Then we go to the measurable function part, and for a R.V X, we focus on the $\sigma$-field it generates, $\sigma(X)$.

Secondly, we have grouping on a collection of independent $\sigma$-field

* If we divide an index set into mutually disjoint subsets, then the $\sigma$-fields generated associated to these subset index are independent.

Thirdly, we have the 0-1 laws.

* Borel: for independent events $\{A_n\}$,if we want to study its limsup, $[A_n\; i.o.]$, we can try study $\sum_{n}P(A_n)$
* Kolmogorov: for independent R.Vs $\{X_n\}$, the information from infinity is almost trivial, and independent to the information of all the $\{X_n\}$.

### Ex 10
::: {.problem}
Suppose $\{X_n,\ n\ge 1\}$ are independent random variables. Show that
\[
P\!\left(\sup_{n\ge 1} X_n < \infty\right)=1
\quad\text{iff}\quad
\sum_{n=1}^\infty P(X_n > M) < \infty \ \text{for some } M.
\]
:::
* My idea: $\Rightarrow$: Suppose $\forall$ $m\in R^+$, we have $\sum_{n}P[X_n>m]=\infty$, that is, we have $P[X_n>m ,\: \text{i.o.}]=1$, also, $[X_n=\infty]=\bigcap_{m}[X_n>m]$, that is, $P[X_n=\infty,\:\text{i.o.}]=1$, which is contradicted to $\sum_n P[\sup_n X_n<\infty]=1$.
$\Leftarrow$: now we have for some $M$, $\sum_{n}P[X_n>M]<\infty$, according to Borel 0-1 law, we have $P[X_n>M,\: \text{i.o.}]=0$, that is $P[\lim\inf[X_n\leq M]]=1$, so there $\exists N$, such that for $n\geq N$, we have $P[X_n\leq M]=1$ therefore we have $P[\sup X_n<\infty]=1$

* Answer: $\Rightarrow$: Suppose $\forall m\in N^+$, we have $\sum_{n}P[X_n>m]=\infty$, that is, by Borel–Cantelli II (using independence), we have $P[X_n> m ,\: \text{i.o.}]=1$, that is, $P(\bigcap_{m=1}^\infty\{X_n> m,\:\text{i.o.}\})=1$ and $\bigcap_{m=1}[X_n> m,\:\text{i.o.}]\subset \{\omega,\:\sup X_n(\omega)=\infty\}$, since $\{\omega,\:\sup X_n(\omega)=\infty\}$ means: for any $m\geq0$, exists $n(\omega,m)>0$, such that $X_n(\omega)>m$, written in set form, it is: $\bigcap_m\bigcup_n[X_n>m]$, so we have $P(\{\omega,\:\sup X_n(\omega)=\infty\})=1$, which leads to contradiction.
$\Leftarrow$: now we have for some $M$, $\sum_{n}P[X_n>M]<\infty$, according to Borel-contelli, $P[X_n>M,\:\text{i.o.}]=0$, that is $P(\lim\inf_n[X_n\leq M])=1$, that is, $P(\bigcup_{N\geq1}\bigcap_{n\geq N}\{\omega,\:X_n(\omega)\leq M\})=1$, so if $\omega\in\bigcup_{N\geq1}\bigcap_{n\geq N}\{\omega,\:X_n(\omega)\leq M\}$, then for this fixed $\omega$, exists a $N(\omega)$, such that for $n\geq N(\omega)$, we have $X_n(\omega)\leq M$, so for a.e $\omega$, we have $\sup_n X_n(\omega)=\max(\max_{1\leq n<N(\omega)}X_n(\omega),M)<\infty$.

* Notes:
  
  1. in general, we don't swap the order of $\bigcap$ and $\bigcup$.
  2. $\sup_n X_n=\infty$ in $\varepsilon-N$ language: for any $m>0$, exists an $n(\omega,m)$, such that $X_n>m$.
  
### Ex 11 
::: {.problem}
Use the Borel--Cantelli Lemma to prove that, given any sequence of random
variables $\{X_n,\ n\ge 1\}$ whose range is the real line, there exist constants $c_n\to\infty$
such that
\[
P\!\left(\lim_{n\to\infty}\frac{X_n}{c_n}=0\right)=1.
\]
Give a careful description of how you choose $c_n$.
:::

* My idea: 
if we want to find $\{c_n\}$ that $P[\lim_{n\to\infty}\frac{X_n}{c_n}=0]$, think the opposite, that is, for any $a>0$, we must have $P[\frac{X_n}{c_n}>a,\ \text{i.o.}]=0$, because if such $\{c_n\}$ exists, $X_n$ would not exceed any positive number infintely many times, so we just need to fix some $a>0$, and construct $\{c_n\}$ such that make sure $\sum_{n=1}^{\infty} P[X_n>a\cdot c_n]<\infty$, so I try to make it shrink as $\frac{1}{2^n}$, then I may choose $c_n=\sup_n X_n\cdot2^n$.

* Answer:
We need to guarantee that for any $k\in N^+$, $P[\frac{X_n}{c_n}>\frac{1}{k}\ \text{i.o.}]=0$ so that $\frac{X_n}{c_n}\to0\ \text{a.s.}$, since $\{\frac{X_n}{c_n}\not\to0\}=\bigcup_{k\geq1}\{\frac{X_n}{c_n}>\frac{1}{k}\ \text{i.o.}\}$, according to Borel-contelli I, we start from $\sum_{n=1}^{\infty} P(|X_n|>\frac{c_n}{k}<\infty)$, since $\{X_n\}$ are r.v., so we have $P(|X_n|>t)\to0\ (t\to\infty)$, that is, for $2^{-n}$, exists a $t_n$, such that $P(|X_n|>t_n)\leq2^{-n}$, now if we want $\frac{c_n}{k}\geq t_n$ for any fixed $k\in N^+$, we can set $c_n=n\cdot t_n$, so $\frac{c_n}{k}=\frac{n\cdot t_n}{k}\geq t_n$, so we have $\sum_{n}P(|X_n|>\frac{c_n}{k})<\infty$, thus $P[\lim_n\frac{X_n}{c_n}=0]=1$.

* Notes:
  The diagonal trick: we want for any $k$, $c_n$ satisfies $\frac{c_n}{k}>t_n$, that is we have 2 directions: $n$ and $k$ in a $\infty\times\infty$ matrix, then for each $k$, our $n$ can move along in the triangle $n\geq k$, to make sure that $\frac{n\cdot t_n}{k}\geq t_n$.


